master:
  replicaCount: 3
  resources:
    requests: { cpu: "3000m", memory: "6Gi" }
    limits: { cpu: "6000m", memory: "6Gi" }
  roles:
    master: true
    data: false
    ingest: false
    coordinating: false
  heapSize: 3g
  extraEnvVars:
    - name: ES_JAVA_OPTS
      value: "-XX:G1HeapRegionSize=8m -XX:HeapDumpPath=/tmp"
  persistence:
    enabled: true
    size: 2Gi
  pdb:
    create: true
    minAvailable: 2
  service:
    type: ClusterIP
  # Schedule master pods on dedicated c2-standard-4 nodes in the 'es-master' pool.
  # These nodes are tainted to prevent CI workloads from competing for CPU,
  # which is critical because cluster state computation is single-threaded.
  nodeSelector:
    cloud.google.com/gke-nodepool: es-master
  tolerations:
    - key: "component"
      value: "es-master"
      effect: "NoSchedule"
  podAntiAffinityPreset: "hard"
  readinessProbe:
    enabled: true
    initialDelaySeconds: 45
    periodSeconds: 10
  livenessProbe:
    enabled: true
    initialDelaySeconds: 90
    periodSeconds: 20

data:
  replicaCount: 6
  heapSize: 2g
  resources:
    requests: { cpu: "2000m", memory: "4Gi" }
    limits: { cpu: "4000m", memory: "4Gi" }
  roles:
    master: false
    data: true
    ingest: false
    coordinating: false
  persistence:
    enabled: true
    size: 20Gi
  pdb:
    create: true
    minAvailable: 4
  podAntiAffinityPreset: "soft"
  readinessProbe:
    enabled: true
    initialDelaySeconds: 45
    periodSeconds: 10
  livenessProbe:
    enabled: true
    initialDelaySeconds: 90
    periodSeconds: 20

coordinating:
  replicaCount: 2
  heapSize: 2g
  resources:
    requests: { cpu: "1500m", memory: "2Gi" }
    limits: { cpu: "3000m", memory: "4Gi" }
  roles:
    master: false
    data: false
    ingest: false
    coordinating: true
    remote_cluster_client: true
  persistence:
    enabled: false
  service:
    type: ClusterIP
    ports:
      - name: https
        port: 9200
        targetPort: 9200
  pdb:
    create: true
    minAvailable: 1
  podAntiAffinityPreset: "soft"
  readinessProbe:
    enabled: true
    initialDelaySeconds: 45
    periodSeconds: 10
  livenessProbe:
    enabled: true
    initialDelaySeconds: 90
    periodSeconds: 20

ingest:
  replicaCount: 2
  heapSize: 2g
  resources:
    requests: { cpu: "1000m", memory: "2Gi" }
    limits: { cpu: "2000m", memory: "4Gi" }
  roles:
    master: false
    data: false
    ingest: true
    coordinating: false
  persistence:
    enabled: false
  pdb:
    create: true
    minAvailable: 1
  podAntiAffinityPreset: "soft"
  readinessProbe:
    enabled: true
    initialDelaySeconds: 45
    periodSeconds: 10
  livenessProbe:
    enabled: true
    initialDelaySeconds: 90
    periodSeconds: 20

global:
  security:
    allowInsecureImages: true
  kibanaEnabled: true

kibana:
  image:
    repository: bitnamilegacy/kibana
    tag: 8.18.0
  resources:
    requests: { cpu: "500m", memory: "2Gi" }
    limits: { cpu: "2000m", memory: "3Gi" }
  extraEnvVars:
    - name: NODE_OPTIONS
      value: "--max-old-space-size=3072"
  extraConfiguration:
    xpack.reporting.enabled: false
    xpack.fleet.enabled: true
    # Required by Kibana alerting, connectors, and saved object encryption.
    # Without this key Kibana refuses to create alerting rules or actions.
    xpack.encryptedSavedObjects.encryptionKey: "5470b74cd2be8ca2a9d6f0dbc0d34e89"
    # --- Stack Monitoring ---
    # Enables the Stack Monitoring UI in Kibana so you can view cluster health,
    # node metrics, index performance, and shard allocation over time.
    monitoring.ui.enabled: true
    # Use the self-monitoring data written by ES to local .monitoring-* indices
    # (collection is enabled via xpack.monitoring.collection.enabled on the ES side).
    monitoring.ui.ccs.enabled: false
  elasticsearch:
    security:
      auth:
        enabled: true
        existingSecret: infra-credentials
        elasticsearchPasswordSecret: infra-credentials
        createSystemUser: true
      tls:
        enabled: true
        existingSecret: elasticsearch-coordinating-crt
        usePemCerts: true
  ingress:
    enabled: true
    ingressClassName: nginx
    hostname: kibana-es-21-6-3.ci.distro.ultrawombat.com
    path: /
    tls: true
    annotations:
      nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
      nginx.ingress.kubernetes.io/proxy-ssl-verify: "off"
      nginx.ingress.kubernetes.io/proxy-body-size: "20m"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    extraTls:
      - hosts:
          - kibana-es-21-6-3.ci.distro.ultrawombat.com
        secretName: camunda-platform

security:
  enabled: true
  existingSecret: infra-credentials
  tls:
    autoGenerated: true
    restEncryption: true
    verificationMode: certificate

sysctlImage:
  repository: bitnamilegacy/os-shell
  pullPolicy: IfNotPresent

copyTlsCerts:
  image:
    repository: bitnamilegacy/os-shell
    pullPolicy: IfNotPresent

image:
  repository: bitnamilegacy/elasticsearch
  tag: 8.18.0

extraConfig:
  logger.org.elasticsearch.deprecation: "OFF"
  cluster.max_shards_per_node: 3500
  cluster.routing.allocation.disk.threshold_enabled: true
  cluster.routing.allocation.disk.watermark.low: 70%
  cluster.routing.allocation.disk.watermark.high: 85%
  cluster.routing.allocation.disk.watermark.flood_stage: 90%
  action.auto_create_index: true
  action.destructive_requires_name: false
  cluster.routing.allocation.enable: all
  cluster.routing.rebalance.enable: all
  cluster.routing.allocation.same_shard.host: true
  indices.recovery.max_bytes_per_sec: 100mb
  cluster.routing.allocation.node_concurrent_recoveries: 4
  cluster.routing.allocation.node_initial_primaries_recoveries: 8
  # Give the master more time to process and publish cluster state updates.
  # After template cleanup, each update takes ~121ms (14ms computation + 42ms
  # publication + 22ms commit + 38ms completion). ~248 updates can queue before
  # the 30s client timeout. 180s gives headroom if templates accumulate again.
  cluster.publish.timeout: 180s
  # Log cluster state updates that take longer than 10s (default) for diagnostics.
  cluster.service.slow_task_logging_threshold: 5s
  # --- Self-monitoring for Kibana Stack Monitoring ---
  # Enables ES to collect its own metrics (node stats, index stats, cluster stats)
  # and write them to local .monitoring-* indices. Kibana reads these indices to
  # power the Stack Monitoring UI (cluster health, node performance, shard allocation, etc.).
  # NOTE: Must use nested YAML (not flat dotted keys) because the Bitnami chart
  # generates its own nested xpack.security block. Flat "xpack.monitoring.*" keys
  # get clobbered during the YAML merge; nested keys merge correctly.
  xpack:
    monitoring:
      collection:
        enabled: true
        # Collect metrics every 30s (default: 10s). Reduced frequency to limit
        # monitoring index overhead on a CI cluster with many transient indices.
        interval: 30s
      history:
        # Retain monitoring data for 3 days (default: 7d). Keeps disk usage in
        # check while still providing enough history to diagnose multi-hour issues.
        duration: 3d

extraEnvVars:
  - name: ES_JAVA_OPTS
    value: "-XX:G1HeapRegionSize=4m -XX:HeapDumpPath=/tmp"

updateStrategy:
  type: RollingUpdate

ingress:
  enabled: true
  ingressClassName: nginx
  hostname: elasticsearch-21-6-3.ci.distro.ultrawombat.com
  path: /
  tls: true
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
    nginx.ingress.kubernetes.io/proxy-ssl-verify: "off"
    nginx.ingress.kubernetes.io/proxy-body-size: "20m"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
  extraTls:
    - hosts:
        - elasticsearch-21-6-3.ci.distro.ultrawombat.com
      secretName: camunda-platform
